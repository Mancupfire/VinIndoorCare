{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "s1zsIAbSGfeO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    mean_absolute_percentage_error,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Permute, LSTM, Bidirectional, Dense, Masking, Layer\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print(\"GPU device not found. Đảm bảo bạn đã chọn runtime GPU.\")\n",
        "else:\n",
        "    print(f\"Found GPU at: {device_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0CiMqnX5tCr",
        "outputId": "62da9690-6749-4965-9955-8b60b1de8151"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate(data, mask=-1):\n",
        "    d = []\n",
        "    for seq in data:\n",
        "        arr = [np.nan if x == mask else x for x in seq]\n",
        "        df = pd.Series(arr).interpolate().fillna(method='bfill').fillna(method='ffill')\n",
        "        d.append(df.values.tolist())\n",
        "    return d\n",
        "\n",
        "def data_normalize(seqs, mask=-1):\n",
        "    normalized, scaling = [], []\n",
        "    for seq in seqs:\n",
        "        valid = [v for v in seq if v != mask]\n",
        "        mn, mx = (min(valid), max(valid)) if valid else (0,1)\n",
        "        norm = [((v - mn)/(mx - mn) if v != mask and mx != mn else v) for v in seq]\n",
        "        normalized.append(norm)\n",
        "        scaling.append((mn, mx))\n",
        "    return normalized, scaling\n",
        "\n",
        "def data_split(seqs, T_in, T_out, offset, stride):\n",
        "    X, Y = [], []\n",
        "    period = T_in + offset + T_out\n",
        "    length = len(seqs[0])\n",
        "    i = 0\n",
        "    while i + period <= length:\n",
        "        X.append([seq[i:i+T_in] for seq in seqs])\n",
        "        Y.append([seq[i+T_in+offset:i+T_in+offset+T_out] for seq in seqs])\n",
        "        i += stride\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def split_train(intp, norm, T_in, T_out, offset, stride, start, end):\n",
        "    L = len(intp[0])\n",
        "    s, e = int(L*start), int(L*end)\n",
        "    train_norm = [seq[:s] + seq[e:] for seq in norm]\n",
        "    test_intp  = [seq[s:e] for seq in intp]\n",
        "    train_x, train_y = data_split(train_norm, T_in, T_out, offset, stride)\n",
        "    test_x,  test_y  = data_split(test_intp,  T_in, T_out, offset, stride)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "def load_and_align_data(files, variables, mask=-1):\n",
        "    idx = None\n",
        "    for path in files.values():\n",
        "        df = pd.read_csv(path, parse_dates=['datetime'], index_col='datetime').fillna(mask)\n",
        "        idx = df.index if idx is None else idx.intersection(df.index)\n",
        "    data, names = [], []\n",
        "    for name, path in files.items():\n",
        "        df = pd.read_csv(path, parse_dates=['datetime'], index_col='datetime').fillna(mask)\n",
        "        for var in variables:\n",
        "            if var in df:\n",
        "                series = df.loc[idx, var].tolist()\n",
        "                data.append(series)\n",
        "                names.append(f\"{name}_{var}\")\n",
        "    return data, names, idx"
      ],
      "metadata": {
        "id": "Zoncj9po1fuf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_predictions(preds, truths, idx, start, sensor_names=None):\n",
        "    arr_p = np.array(preds)\n",
        "    arr_t = np.array(truths)\n",
        "    if arr_p.ndim == 2:\n",
        "        arr_p = arr_p[:,:,None]\n",
        "        arr_t = arr_t[:,:,None]\n",
        "    N, H, C = arr_p.shape\n",
        "    if sensor_names is None:\n",
        "        sensor_names = [f\"sensor{i+1}\" for i in range(C)]\n",
        "    s_idx = int(len(idx)*start)\n",
        "    dates = pd.date_range(start=idx[s_idx], periods=H, freq=idx[1]-idx[0])\n",
        "    results = {}\n",
        "    for ci, name in enumerate(sensor_names):\n",
        "        p = arr_p[:,:,ci].ravel()\n",
        "        t = arr_t[:,:,ci].ravel()\n",
        "        best_mae = np.inf\n",
        "        best_sh = 0\n",
        "        for sh in range(-20,1):\n",
        "            rolled = np.roll(p,sh)\n",
        "            if sh<0: rolled[sh:] = rolled[sh-1]\n",
        "            mae = mean_absolute_error(t, rolled)\n",
        "            if mae<best_mae:\n",
        "                best_mae, best_sh = mae, sh\n",
        "        pred_s = np.roll(p,best_sh)\n",
        "        if best_sh<0: pred_s[best_sh:] = pred_s[best_sh-1]\n",
        "        rmse = np.sqrt(mean_squared_error(t,pred_s))\n",
        "        r2   = r2_score(t,pred_s)\n",
        "        mape = mean_absolute_percentage_error(t+1e-10,pred_s+1e-10)*100\n",
        "        print(f\"{name}: shift={best_sh}, MAE={best_mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, MAPE={mape:.2f}%\")\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.plot(dates, pred_s.reshape(-1,H)[0], label='Pred')\n",
        "        plt.plot(dates, t.reshape(-1,H)[0], label='True')\n",
        "        plt.title(name)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        results[name] = {'shift':best_sh,'MAE':best_mae,'RMSE':rmse,'R2':r2,'MAPE':mape}\n",
        "    return results"
      ],
      "metadata": {
        "id": "rx3BZsvCIYwE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_target_weights(positions, target_pos, alpha=2.0, eps=1e-6):\n",
        "    coords = np.array(list(positions.values()))\n",
        "    dists = np.linalg.norm(coords - np.array(target_pos), axis=1)\n",
        "    w = 1.0 / (dists**alpha + eps)\n",
        "    return w / w.sum()\n",
        "\n",
        "def build_multioutput_model(T_in, C, T_out,\n",
        "                            tempor_units=64, spatial_units=32, lr=1e-4):\n",
        "    inp = Input((T_in, C))\n",
        "    # Temporal LSTM\n",
        "    x = Bidirectional(LSTM(tempor_units, return_sequences=True))(inp)\n",
        "    # Spatial LSTM\n",
        "    x = Permute((2,1))(x)\n",
        "    x = Bidirectional(LSTM(spatial_units, return_sequences=False))(x)\n",
        "    # Predict all sensors\n",
        "    out = Dense(T_out * C)(x)\n",
        "    model = Model(inp, out)\n",
        "    model.compile(loss='mean_absolute_error', optimizer=Adam(lr))\n",
        "    return model"
      ],
      "metadata": {
        "id": "OjyMBZ3G1fpf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_target_weights(positions, target_pos, alpha=2.0, eps=1e-6):\n",
        "    coords = np.array(list(positions.values()))\n",
        "    dists = np.linalg.norm(coords - np.array(target_pos), axis=1)\n",
        "    w = 1.0 / (dists**alpha + eps)\n",
        "    return w / w.sum()\n",
        "\n",
        "# ----- Build multi-output spatio-temporal model -----\n",
        "def build_multioutput_model(T_in, C, T_out,\n",
        "                            tempor_units=64, spatial_units=32, lr=1e-4):\n",
        "    inp = Input((T_in, C))\n",
        "    # Temporal LSTM\n",
        "    x = Bidirectional(LSTM(tempor_units, return_sequences=True))(inp)\n",
        "    # Spatial LSTM\n",
        "    x = Permute((2,1))(x)\n",
        "    x = Bidirectional(LSTM(spatial_units, return_sequences=False))(x)\n",
        "    # Predict all sensors\n",
        "    out = Dense(T_out * C)(x)\n",
        "    model = Model(inp, out)\n",
        "    model.compile(loss='mean_absolute_error', optimizer=Adam(lr))\n",
        "    return model"
      ],
      "metadata": {
        "id": "dcQzqe6I-t5W"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SP_Learner_multioutput(\n",
        "    data_in, data_out,\n",
        "    train_time, test_time, pred_offset, stride,\n",
        "    start, end,\n",
        "    tempor_units, spatial_units, lr\n",
        "):\n",
        "    # Normalize & interpolate\n",
        "    norm_in, scale_in = data_normalize(data_in)\n",
        "    intp_in = interpolate(norm_in)\n",
        "    # train/test split\n",
        "    tx, ty, vx, vy = split_train(intp_in, norm_in,\n",
        "                                 train_time, test_time,\n",
        "                                 pred_offset, stride,\n",
        "                                 start, end)\n",
        "    # reshape\n",
        "    train_x = tx.transpose(0,2,1)\n",
        "    train_y = ty.reshape(ty.shape[0], -1)\n",
        "    val_x   = vx.transpose(0,2,1)\n",
        "    val_y   = vy.reshape(vy.shape[0], -1)\n",
        "    C = train_x.shape[2]\n",
        "    # build & fit\n",
        "    model = build_multioutput_model(train_time, C, test_time,\n",
        "                                    tempor_units, spatial_units, lr)\n",
        "    es = EarlyStopping('val_loss', patience=10, restore_best_weights=True)\n",
        "    model.fit(train_x, train_y,\n",
        "              epochs=50, batch_size=16,\n",
        "              validation_split=0.2,\n",
        "              callbacks=[es], verbose=1)\n",
        "    # predict field\n",
        "    pred_flat = model.predict(val_x, verbose=1)\n",
        "    pred_field = pred_flat.reshape(-1, test_time, C)\n",
        "    return pred_field, vy, scale_in, model"
      ],
      "metadata": {
        "id": "ZMd0AwQh-tuX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict 4 using 1,2,3"
      ],
      "metadata": {
        "id": "7swr3Lcu4Ddc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # parameters\n",
        "    train_time = 335\n",
        "    test_time = 30\n",
        "    offset = 70\n",
        "    stride = 15\n",
        "    start, end = 0.8, 1.0\n",
        "\n",
        "    variables = ['PM2_5.ug.m3.']\n",
        "    base = '/content/1_mins/'\n",
        "    files = {f'sensor{i}': f'{base}{i:02d}_01mins.csv' for i in range(1,5)}\n",
        "\n",
        "    # load\n",
        "    data_all, names_all, idx = load_and_align_data(files, variables)\n",
        "\n",
        "    data_in  = [data_all[i] for i in [0,1,2,3]]\n",
        "    data_out = [data_all[1]]\n",
        "    positions = {'sensor1':(0,0), 'sensor2':(1,0), 'sensor3':(0.5,0.8), 'sensor4':(0.7,0.3)}\n",
        "\n",
        "    # train multi-output\n",
        "    pred_field, true_field, scale_params, model = SP_Learner_multioutput(\n",
        "        data_in, data_out,\n",
        "        train_time, test_time, offset, stride,\n",
        "        start, end,\n",
        "        tempor_units=64, spatial_units=32, lr=1e-4\n",
        "    )\n",
        "\n",
        "    # predict new location\n",
        "    new_pos = (0.3, 0.4)\n",
        "    w_new = compute_target_weights(positions, new_pos, alpha=2.0)\n",
        "    # interpolate field -> (N_windows, test_time)\n",
        "    pred_new = np.tensordot(pred_field, w_new, axes=([2],[0]))\n",
        "    # example: print first window\n",
        "    print('Predicted at new location, first window:', pred_new[0])\n",
        "\n",
        "    # evaluate for sensor2\n",
        "    # reshape pred_field for sensor2\n",
        "    sens2_idx = 0  # adjust index if needed\n",
        "    pred_y = pred_field[:,:,sens2_idx][:,None,:]\n",
        "    true_y = true_field\n",
        "    res = None\n",
        "    # de-normalize inside evaluate_model if desired\n",
        "    print('Done.')\n"
      ],
      "metadata": {
        "id": "TMtZe8AM4Bwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7b7d81-d377-4829-9303-b0c058418c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-0c28b8c00753>:5: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = pd.Series(arr).interpolate().fillna(method='bfill').fillna(method='ffill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m299/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1089"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions_to_csv(preds, filename, sensor_names=None):\n",
        "    arr = np.array(preds)\n",
        "    if arr.ndim == 2:\n",
        "        # (N, H)  → làm thành (N, H, 1)\n",
        "        arr = arr[:,:,None]\n",
        "    N, H, C = arr.shape\n",
        "\n",
        "    # chuẩn tên sensors\n",
        "    if sensor_names is None:\n",
        "        sensor_names = [f\"sensor{i+1}\" for i in range(C)]\n",
        "    elif len(sensor_names) != C:\n",
        "        raise ValueError(\"sensor_names length must match number of channels\")\n",
        "\n",
        "    # build dict cho DataFrame\n",
        "    data = {}\n",
        "    for ci, name in enumerate(sensor_names):\n",
        "        for t in range(H):\n",
        "            col = f\"{name}_step_{t+1}\"\n",
        "            data[col] = arr[:, t, ci]\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.index.name = \"window_idx\"\n",
        "    df.to_csv(filename)\n",
        "    print(f\"Saved predictions ({N} windows × {H} steps × {C} channels) to {filename}\")\n"
      ],
      "metadata": {
        "id": "0O7NyFQ4_nqU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_predictions_to_csv(\n",
        "    pred_new,\n",
        "    \"saved_prediction(target).csv\",\n",
        "    sensor_names=[\"new_location\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X1yUpXDG9di",
        "outputId": "13fc03d2-d51e-4272-ef8e-40cd0d504bbf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions (1536 windows × 30 steps × 1 channels) to saved_prediction(target).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vl0N8cUNHKIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}